{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\udd16 GodotAI","text":"<p>Bienvenue sur la documentation officielle de GodotAI.</p> <p>Ce projet associe Godot, FastAPI et Ollama pour proposer un mini-jeu capable de communiquer avec un mod\u00e8le de langage local. Toute la stack s'ex\u00e9cute dans des conteneurs Docker afin de rester simple \u00e0 lancer.</p> <p>\ud83c\udf1f Retrouvez le d\u00e9p\u00f4t sur GitHub pour explorer le code source.</p>"},{"location":"#apercu-de-larchitecture","title":"Aper\u00e7u de l'architecture","text":"<p>Voici le fonctionnement g\u00e9n\u00e9ral\u00a0: le joueur dialogue avec Godot, qui fait appel au backend FastAPI. Celui-ci interroge Ollama pour le texte et Stable Diffusion pour l'image, puis consigne les \u00e9changes dans SQLite avant de r\u00e9pondre au client.</p> <p></p> <p>Cette documentation suit le cadre Di\u00e1taxis et se divise en quatre sections\u00a0:</p> <ul> <li>Tutoriels \ud83d\udee0\ufe0f : apprenez pas \u00e0 pas \u00e0 installer et utiliser le projet.</li> <li>Guides pratiques \ud83e\uddf0 : r\u00e9pondez \u00e0 un besoin pr\u00e9cis apr\u00e8s l'installation.</li> <li>R\u00e9f\u00e9rence \ud83d\udcda : trouvez la description exhaustive des commandes et fichiers.</li> <li>Explications \ud83e\udde9 : comprenez l'architecture et les choix techniques.</li> </ul>"},{"location":"#acces-rapide","title":"Acc\u00e8s rapide","text":"<ul> <li>Tutoriel de prise en main</li> <li>Guides pratiques</li> <li>R\u00e9f\u00e9rence technique</li> <li>Explications</li> </ul>"},{"location":"#structure-detaillee","title":"Structure d\u00e9taill\u00e9e","text":""},{"location":"#tutoriels","title":"\ud83d\udee0\ufe0f Tutoriels","text":"<ul> <li>\ud83d\ude80 Prise en main</li> </ul>"},{"location":"#guides-pratiques","title":"\ud83e\uddf0 Guides pratiques","text":"<ul> <li>\ud83d\udd04 Changer le mod\u00e8le LLM</li> <li>\u270f\ufe0f Adapter le prompt</li> <li>\ud83d\udce1 Utiliser l'API</li> <li>\u2699\ufe0f Configurer l'environnement</li> <li>\ud83e\ude7a D\u00e9pannage mod\u00e8les et GPU</li> <li>\u270d\ufe0f Contr\u00f4ler la r\u00e9daction avec Vale</li> <li>\ud83d\udee0\ufe0f Troubleshooting</li> </ul>"},{"location":"#reference-technique","title":"\ud83d\udcda R\u00e9f\u00e9rence technique","text":"<ul> <li>\ud83d\udd17 API du backend</li> <li>\u2699\ufe0f Configuration</li> <li>\ud83d\udee0\ufe0f Makefile</li> <li>\ud83d\udc33 docker-compose.yml</li> <li>\ud83d\udcc4 Dockerfile.fastapi</li> <li>\ud83d\udc0b Dockerfile.ollama</li> <li>\ud83d\udd27 entrypoint_ollama.sh</li> <li>\ud83d\uddd2\ufe0f Modelfile</li> <li>\ud83d\udcdc mkdocs.yml</li> <li>\ud83d\udcd1 variables d'environnement</li> <li>\ud83d\udcdd AGENTS.md</li> <li>\ud83d\ude48 .gitignore</li> <li>\u2705 Tests unitaires</li> <li>\ud83d\udea6 Tests E2E</li> <li>\ud83d\udee0\ufe0f test_services.py</li> <li>\ud83d\udd0d Vale</li> </ul>"},{"location":"#explications","title":"\ud83e\udde9 Explications","text":"<ul> <li>\ud83c\udfd7\ufe0f Architecture</li> <li>\ud83d\uddfa\ufe0f Flux complet</li> <li>\ud83d\ude80 Bootstrap</li> <li>\ud83d\udcbb Backend d\u00e9taill\u00e9</li> <li>\u26a1 FastAPI</li> <li>\ud83e\udd16 Ollama</li> <li>\ud83c\udfa8 Stable Diffusion</li> <li>\ud83c\udfae Godot</li> <li>\ud83d\udc0b Docker Compose</li> <li>\ud83d\udcd6 MkDocs</li> </ul> <p>Bonne lecture\u00a0!</p>"},{"location":"explications/architecture/","title":"\ud83e\udde9 Comprendre la stack","text":"<p>Cette page pr\u00e9sente bri\u00e8vement l'architecture g\u00e9n\u00e9rale avant de d\u00e9tailler chaque composant.</p> <p>Le diagramme ci-dessous est g\u00e9n\u00e9r\u00e9 en SVG avec D2\u00a0:</p> <p></p>"},{"location":"explications/architecture/#role-des-composants","title":"R\u00f4le des composants","text":"<ul> <li>Godot \ud83c\udfae : le dossier <code>godot/</code> renferme les sc\u00e8nes et scripts du mini-jeu. La sc\u00e8ne <code>scenes/Main.tscn</code> communique avec l'API via des n\u0153uds <code>HTTPRequest</code>.</li> <li>FastAPI \u26a1 : le backend Python vit dans <code>backend/app</code>. Le module <code>main.py</code> expose notamment la route <code>/txt</code> et enregistre les \u00e9changes dans <code>data/game.db</code> gr\u00e2ce \u00e0 SQLAlchemy.</li> <li>Ollama \ud83e\udd99 : construit via <code>Dockerfile.ollama</code>, ce service cr\u00e9e le mod\u00e8le <code>god</code> \u00e0 partir du <code>Modelfile</code> puis t\u00e9l\u00e9charge au besoin le mod\u00e8le indiqu\u00e9 par <code>OLLAMA_TEXT_MODEL</code> gr\u00e2ce au script <code>entrypoint_ollama.sh</code>.</li> <li>Stable Diffusion \ud83c\udfa8 : le service <code>stablediffusion</code> g\u00e8re la g\u00e9n\u00e9ration d'images et conserve les fichiers dans les volumes <code>sd_models</code> et <code>sd_outputs</code>.</li> <li>Docker Compose \ud83d\udc33 : le fichier <code>docker-compose.yml</code> orchestre tous les conteneurs et le <code>Makefile</code> fournit les raccourcis <code>make up</code> et <code>make down</code>.</li> <li>MkDocs \ud83d\udcda : la documentation statique est g\u00e9n\u00e9r\u00e9e depuis <code>docs/</code> \u00e0 l'aide du fichier <code>mkdocs.yml</code>.</li> </ul>"},{"location":"explications/architecture/#pages-detaillees","title":"Pages d\u00e9taill\u00e9es","text":"<ul> <li>\u26a1 FastAPI</li> <li>\ud83e\udd99 Ollama</li> <li>\ud83c\udfa8 Stable Diffusion</li> <li>\ud83c\udfae Godot</li> <li>\ud83d\udc33 Docker Compose</li> <li>\ud83d\udcda MkDocs</li> </ul> <p>Chaque page de la documentation renvoie vers le site officiel et le manuel de r\u00e9f\u00e9rence pour en apprendre davantage.</p>"},{"location":"explications/architecture/#exemple-dappel-api","title":"Exemple d'appel API","text":"<pre><code>import requests\n\nBASE_URL = \"http://localhost:8000\"\n    resp = requests.post(\n        f\"{BASE_URL}/txt\",\n        json={\"prompt\": \"look around\", \"stream\": False},\n    )\nprint(resp.json())\n</code></pre>"},{"location":"explications/architecture/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Tutoriel de prise en main</li> </ul>"},{"location":"explications/backend/","title":"\ud83d\udcdd Backend d\u00e9taill\u00e9","text":"<p>Cette page explore plus en profondeur les modules Python du dossier <code>backend/app</code>.</p> <p>Le backend utilise FastAPI pour exposer plusieurs routes appel\u00e9es par le jeu Godot. Lorsqu'une action du joueur est re\u00e7ue, elle est envoy\u00e9e \u00e0 Ollama pour produire une r\u00e9ponse puis, si besoin, \u00e0 Stable Diffusion pour g\u00e9n\u00e9rer une illustration. Les \u00e9changes structur\u00e9s sont stock\u00e9s dans SQLite ou PostgreSQL via SQLAlchemy, tandis que les r\u00e9ponses brutes du mod\u00e8le sont archiv\u00e9es dans MongoDB.</p>"},{"location":"explications/backend/#mainpy","title":"main.py","text":"<p>Ce fichier instancie FastAPI et expose les routes principales du projet. Il g\u00e8re la g\u00e9n\u00e9ration de texte avec Ollama et la cr\u00e9ation d'images via Stable Diffusion. Il inclut \u00e9galement le routeur <code>mcp.py</code> qui impl\u00e9mente un petit protocole JSON-RPC (un format d'appel distant bas\u00e9 sur JSON).</p>"},{"location":"explications/backend/#mcppy","title":"mcp.py","text":"<p>Le module <code>mcp.py</code> d\u00e9finit un routeur FastAPI d\u00e9di\u00e9 au protocole MCP. Il permet de d\u00e9crire les outils et ressources mis \u00e0 disposition par le backend et g\u00e8re les requ\u00eates JSON-RPC entrantes.</p>"},{"location":"explications/backend/#modelspy-et-databasepy","title":"models.py et database.py","text":"<p>La persistance s'appuie sur SQLAlchemy avec une base SQLite par d\u00e9faut ou PostgreSQL si configur\u00e9. Les mod\u00e8les <code>User</code>, <code>Session</code> et <code>Message</code> d\u00e9crivent la structure principale des donn\u00e9es du jeu. Les r\u00e9ponses JSON compl\u00e8tes sont sauvegard\u00e9es dans MongoDB gr\u00e2ce au module <code>mongo_database.py</code>.</p>"},{"location":"explications/backend/#embedding_contextpy","title":"embedding_context.py","text":"<p><code>EmbeddingContext</code> conserve en m\u00e9moire les derniers messages d'une session afin de fournir un contexte lors des \u00e9changes avec le mod\u00e8le de langage.</p>"},{"location":"explications/backend/#ollama_clientpy","title":"ollama_client.py","text":"<p>Ce module regroupe les appels HTTP n\u00e9cessaires pour interroger Ollama afin de g\u00e9n\u00e9rer du texte. Il est invoqu\u00e9 par le serveur lorsqu'une r\u00e9ponse conversationnelle est demand\u00e9e.</p>"},{"location":"explications/backend/#stablediffusion_clientpy","title":"stablediffusion_client.py","text":"<p>Cette nouvelle unit\u00e9 se charge d'appeler Stable Diffusion pour produire les illustrations. Elle est isol\u00e9e afin de ne plus m\u00ealer la logique de g\u00e9n\u00e9ration d'images avec celle du texte.</p>"},{"location":"explications/backend/#configpy","title":"config.py","text":"<p>Toutes les variables de configuration (h\u00f4tes, ports, nom des mod\u00e8les Ollama) proviennent de ce fichier. Les valeurs peuvent \u00eatre surcharg\u00e9es via un fichier <code>.env</code>.</p> <p>En r\u00e9sum\u00e9, le backend combine FastAPI, SQLAlchemy et plusieurs services externes pour orchestrer la partie narrative du mini-jeu Godot. L'ensemble est d\u00e9marr\u00e9 par Docker Compose et reste facilement extensible.</p>"},{"location":"explications/backend/#voir-aussi","title":"Voir aussi","text":"<ul> <li>R\u00e9f\u00e9rence de l'API</li> <li>Explications sur FastAPI</li> </ul>"},{"location":"explications/bootstrap/","title":"\ud83d\ude80 Bootstrap de l'application","text":"<p>Ce sch\u00e9ma retrace les \u00e9tapes effectu\u00e9es lors d'un <code>make up</code>.</p> <pre><code>make up\n  |\n  `- docker compose up\n       |\n       |-- Construction des images\n       |    |- fastapi : Dockerfile -&gt; uvicorn\n       |    |- ollama : Dockerfile.ollama -&gt; entrypoint_ollama.sh\n       |    `- stablediffusion : image pr\u00e9existante\n       |\n       `-- D\u00e9marrage des conteneurs\n             |\n             |-&gt; ollama\n             |     `-&gt; entrypoint_ollama.sh\n             |          - lance \"ollama serve\"\n             |          - attend que l'API soit pr\u00eate\n             |          - `ollama pull` des mod\u00e8les manquants\n             |\n             |-&gt; stablediffusion\n             |     - t\u00e9l\u00e9charge les poids si n\u00e9cessaire\n             |\n             `-&gt; fastapi\n                   - ex\u00e9cute uvicorn avec main\n</code></pre>"},{"location":"explications/bootstrap/#detail-des-etapes","title":"D\u00e9tail des \u00e9tapes","text":"<ol> <li>Commande <code>make up</code> : lance <code>docker compose up -d</code> en utilisant les variables du fichier <code>.env</code> pour choisir les ports et les mod\u00e8les.</li> <li>Construction des images : <code>fastapi</code> et <code>ollama</code> sont (re)construits si n\u00e9cessaire, <code>stablediffusion</code> est r\u00e9cup\u00e9r\u00e9e depuis Docker Hub.</li> <li>Lancement d'<code>ollama</code> : le script <code>entrypoint_ollama.sh</code> d\u00e9marre <code>ollama serve</code>, v\u00e9rifie la pr\u00e9sence des mod\u00e8les et les t\u00e9l\u00e9charge au besoin.</li> <li>Lancement de <code>stablediffusion</code> : l'image pr\u00e9voit un script similaire pour r\u00e9cup\u00e9rer les poids avant de d\u00e9marrer la WebUI.</li> <li>Lancement de <code>fastapi</code> : une fois les services pr\u00e9c\u00e9dents disponibles, Uvicorn ex\u00e9cute <code>backend.app.main:app</code>.</li> <li>Application pr\u00eate : Godot ou tout autre client peut d\u00e9sormais appeler l'API.</li> </ol>"},{"location":"explications/bootstrap/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Tutoriel de d\u00e9marrage</li> </ul>"},{"location":"explications/docker-compose/","title":"\ud83d\udc33 Docker Compose","text":"<p>Docker Compose est l'outil qui lance plusieurs conteneurs Docker en une seule commande. Le fichier <code>docker-compose.yml</code> d\u00e9finit d\u00e9sormais cinq services\u00a0: fastapi, ollama, stablediffusion, postgres et mongo.</p> <p></p> <p>Pour simplifier la vie du d\u00e9veloppeur, toutes les commandes utiles sont regroup\u00e9es dans le <code>Makefile</code>.</p> <p>Commandes utiles : <pre><code># D\u00e9marrer les services en arri\u00e8re-plan\nmake up\n\n# Arr\u00eater l'ensemble\nmake down\n</code></pre></p>"},{"location":"explications/docker-compose/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Fichier <code>docker-compose.yml</code></li> <li>Makefile</li> </ul>"},{"location":"explications/docker-compose/#ressources","title":"Ressources","text":"<ul> <li>Site officiel</li> <li>Documentation</li> </ul>"},{"location":"explications/fastapi/","title":"\u26a1 FastAPI","text":"<p>FastAPI est un framework Web moderne et asynchrone pour Python. Il s'appuie sur Pydantic pour la validation des donn\u00e9es et g\u00e9n\u00e8re automatiquement une documentation interactive.</p> <p>Dans GodotAI, FastAPI sert de colonne vert\u00e9brale au backend : il expose les routes appel\u00e9es par Godot, dialogue avec Ollama pour produire du texte et d\u00e9clenche la g\u00e9n\u00e9ration d'images via Stable Diffusion. Les donn\u00e9es sont enregistr\u00e9es dans SQLite ou PostgreSQL, et les r\u00e9ponses compl\u00e8tes du mod\u00e8le sont \u00e9galement consign\u00e9es dans MongoDB.</p> <p>Le service tourne dans son propre conteneur Docker, d\u00e9marr\u00e9 avec <code>make up</code>. Il redirige les requ\u00eates vers Ollama via l'endpoint <code>/txt</code> et vers Stable Diffusion via <code>/img</code>.</p> <p></p>"},{"location":"explications/fastapi/#exemple-minimal","title":"Exemple minimal","text":"<pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/ping\")\ndef ping():\n    return {\"message\": \"pong\"}\n</code></pre>"},{"location":"explications/fastapi/#voir-aussi","title":"Voir aussi","text":"<ul> <li>R\u00e9f\u00e9rence de l'API</li> <li>Guide d'utilisation de l'API</li> </ul>"},{"location":"explications/fastapi/#ressources","title":"Ressources","text":"<ul> <li>Site officiel</li> <li>Documentation</li> </ul>"},{"location":"explications/flux-global/","title":"\ud83d\uddfa\ufe0f Flux complet du projet","text":"<p>Ce sch\u00e9ma r\u00e9sume toutes les \u00e9tapes principales, du clonage du d\u00e9p\u00f4t \u00e0 l'utilisation par le joueur.</p> <p></p> <ol> <li>Clonage et installation : <code>make install</code> installe les d\u00e9pendances Python.</li> <li>Construction : <code>docker compose build</code> pr\u00e9pare les images pour FastAPI et Ollama.</li> <li>R\u00e9cup\u00e9ration des mod\u00e8les : lors du premier lancement, Ollama et Stable Diffusion t\u00e9l\u00e9chargent les fichiers n\u00e9cessaires.</li> <li>G\u00e9n\u00e9ration de la doc : <code>mkdocs build</code> transforme les fichiers Markdown en site web.</li> <li>Ex\u00e9cution : <code>docker compose up</code> d\u00e9marre les services pour que Godot puisse interroger l'API.</li> </ol>"},{"location":"explications/flux-global/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Bootstrap</li> <li>MkDocs</li> </ul>"},{"location":"explications/godot/","title":"\ud83c\udfae Godot","text":"<p>Godot est un moteur de jeu libre et l\u00e9ger. Dans ce projet, il fournit l'interface graphique du mini-jeu et communique avec l'API Python. Les scripts GDScript appellent l'endpoint <code>/txt</code> pour afficher les r\u00e9ponses du mod\u00e8le. Chaque zone de l'\u00e9cran est un panneau pouvant \u00eatre d\u00e9plac\u00e9 gr\u00e2ce au script <code>DraggablePanel.gd</code>.</p> <p>Quand le joueur effectue une action, ces scripts envoient la requ\u00eate \u00e0 FastAPI qui renvoie le texte g\u00e9n\u00e9r\u00e9 par Ollama.</p> <p></p> <p>Extrait de la fonction d'envoi d'un message au mod\u00e8le\u00a0:</p> <pre><code>func _send_to_llm(message: String):\n    var url := \"http://localhost:8000/txt\"\n    var body := {\n        \"model\": \"god:latest\",\n        \"prompt\": message,\n        \"stream\": false\n    }\n    var headers := [\"Content-Type: application/json\"]\n    var err := http.request(url, headers, HTTPClient.METHOD_POST, JSON.stringify(body))\n    if err != OK:\n        _append_message(\"Assistant\", \"[Erreur r\u00e9seau]\")\n</code></pre> <p>Pour lancer l'\u00e9diteur : <pre><code>make godot\n</code></pre></p>"},{"location":"explications/godot/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Utiliser l'API REST</li> <li>Architecture globale</li> </ul>"},{"location":"explications/godot/#ressources","title":"Ressources","text":"<ul> <li>Site officiel</li> <li>Documentation</li> </ul>"},{"location":"explications/mkdocs/","title":"\ud83d\udcda MkDocs","text":"<p>MkDocs transforme les fichiers Markdown du dossier <code>docs/</code> en un site statique pr\u00eat \u00e0 \u00eatre publi\u00e9. Le th\u00e8me Material apporte un rendu moderne et agr\u00e9able.</p> <p>Les diagrammes D2 sont convertis en SVG via une action GitHub, puis inclus directement dans la documentation.</p> <p></p> <p>Pour tester en local : <pre><code>make install\nmake docs-serve\n</code></pre></p>"},{"location":"explications/mkdocs/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Fichier <code>mkdocs.yml</code></li> <li>Contr\u00f4ler la r\u00e9daction avec Vale</li> </ul>"},{"location":"explications/mkdocs/#ressources","title":"Ressources","text":"<ul> <li>Site officiel</li> <li>Documentation</li> </ul>"},{"location":"explications/ollama/","title":"\ud83e\udd99 Ollama","text":"<p>Ollama lance le mod\u00e8le de langage \u00e0 l'int\u00e9rieur d'un conteneur Docker. Il t\u00e9l\u00e9charge le mod\u00e8le choisi lors du premier d\u00e9marrage (par exemple Mistral ou Llama\u00a03). Lorsque vous ex\u00e9cutez <code>make up</code>, le conteneur lance le script <code>entrypoint_ollama.sh</code> qui v\u00e9rifie la pr\u00e9sence des mod\u00e8les et les t\u00e9l\u00e9charge automatiquement s'ils sont absents.</p> <p>Le processus est pilot\u00e9 par le script <code>entrypoint_ollama.sh</code>\u00a0: 1. Il d\u00e9marre <code>ollama serve</code> en arri\u00e8re-plan et attend que l'API r\u00e9ponde. 2. Il v\u00e9rifie la pr\u00e9sence des mod\u00e8les list\u00e9s dans les variables d'environnement    <code>OLLAMA_TEXT_MODEL</code> et <code>STABLEDIFFUSION_MODEL</code> via <code>ollama list</code>. 3. Si l'un d'eux est absent, <code>ollama pull</code> est ex\u00e9cut\u00e9 pour le t\u00e9l\u00e9charger. 4. Enfin, le script laisse tourner <code>ollama serve</code> pour traiter les requ\u00eates.</p> <p>Les mod\u00e8les restent dans le volume Docker <code>ollama_models</code>, \u00e9vitant ainsi de nouveaux t\u00e9l\u00e9chargements aux lancements suivants.</p> <p>Le fichier <code>Modelfile</code> \u00e0 la racine du d\u00e9p\u00f4t indique quel mod\u00e8le charger. FastAPI lui envoie les requ\u00eates de l'utilisateur pour obtenir une r\u00e9ponse adapt\u00e9e \u00e0 la partie en cours.</p> <p>Lors de la construction de l'image, cette configuration est transform\u00e9e en un mod\u00e8le nomm\u00e9 <code>god</code>. On d\u00e9marre donc temporairement le serveur Ollama\u00a0:</p> <pre><code>ollama serve &amp;\nuntil curl -s http://127.0.0.1:11434/api/tags &gt; /dev/null; do sleep 1; done\nollama create god -f /Modelfile\nkill $!\n</code></pre> <p>Le mod\u00e8le est stock\u00e9 dans le volume <code>ollama_models</code> pour \u00e9viter de nouveaux t\u00e9l\u00e9chargements.</p>"},{"location":"explications/ollama/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Fichier <code>Modelfile</code></li> <li>Changer de mod\u00e8le</li> </ul> <p>Exemple d'ex\u00e9cution manuelle : <pre><code>docker run -p 11434:11434 ollama/ollama:latest serve\n</code></pre></p>"},{"location":"explications/ollama/#ressources","title":"Ressources","text":"<ul> <li>Site officiel</li> <li>Documentation</li> </ul>"},{"location":"explications/stable-diffusion/","title":"\ud83c\udfa8 Stable Diffusion","text":"<p>Stable Diffusion est un mod\u00e8le de g\u00e9n\u00e9ration d'images \u00e0 partir d'une description textuelle. Dans ce projet, il s'ex\u00e9cute dans un conteneur d\u00e9di\u00e9. Au premier <code>make up</code>, le conteneur t\u00e9l\u00e9charge automatiquement les poids du mod\u00e8le puis les conserve pour les ex\u00e9cutions suivantes.</p> <p>L'image Docker embarque un script de d\u00e9marrage qui v\u00e9rifie si les fichiers sont pr\u00e9sents dans le volume <code>sd_models</code>. Si ce n'est pas le cas, ils sont t\u00e9l\u00e9charg\u00e9s avant que l'interface WebUI ne se lance.</p> <p>FastAPI lui transmet vos invites afin d'illustrer certaines sc\u00e8nes du jeu.</p> <p></p> <p>Vous pouvez g\u00e9n\u00e9rer une image directement via l'API. Assurez\u2011vous simplement que les conteneurs sont d\u00e9marr\u00e9s avec <code>make up</code>\u00a0: <pre><code>curl -X POST http://localhost:8000/img \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"prompt\": \"un village m\u00e9di\u00e9val\"}' -o output.png\n</code></pre></p>"},{"location":"explications/stable-diffusion/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Configuration des services</li> <li>Guide d'utilisation de l'API</li> </ul>"},{"location":"explications/stable-diffusion/#ressources","title":"Ressources","text":"<ul> <li>Site officiel</li> <li>Documentation</li> </ul>"},{"location":"guides/","title":"Guides pratiques","text":"<p>Ces guides r\u00e9pondent \u00e0 des besoins pr\u00e9cis une fois l'installation termin\u00e9e.</p> <ul> <li>\ud83d\udd04 Changer le mod\u00e8le LLM : t\u00e9l\u00e9charger et utiliser une autre base.</li> <li>\u270f\ufe0f Adapter le prompt : personnaliser le prompt syst\u00e8me.</li> <li>\ud83d\udce1 Utiliser l'API : interagir avec FastAPI sans passer par Godot.</li> <li>\u2699\ufe0f Configurer l'environnement : d\u00e9finir les variables dans <code>.env</code>.</li> <li>\ud83e\ude7a D\u00e9pannage mod\u00e8les et GPU : r\u00e9soudre les soucis de d\u00e9marrage ou de ressources.</li> <li>\u270d\ufe0f Contr\u00f4ler la r\u00e9daction avec Vale : v\u00e9rifier la coh\u00e9rence des articles.</li> <li>\ud83d\udee0\ufe0f Troubleshooting : r\u00e9soudre les erreurs courantes.</li> </ul>"},{"location":"guides/adapter-prompt/","title":"Adapter le prompt syst\u00e8me","text":"<p>Le fichier <code>Modelfile</code> d\u00e9finit le prompt syst\u00e8me envoy\u00e9 au mod\u00e8le lors de chaque g\u00e9n\u00e9ration. Pour le personnaliser\u00a0:</p> <ol> <li>\u00c9ditez <code>Modelfile</code> et modifiez la section <code>SYSTEM</code> selon vos besoins.</li> <li>Si vous changez d'autres param\u00e8tres (par exemple <code>PARAMETER temperature 0.7</code>), enregistrez le fichier.</li> <li>Red\u00e9marrez Ollama pour appliquer les modifications\u00a0:    <pre><code>make down\nmake up\n</code></pre>    Le prompt modifi\u00e9 sera pris en compte lors des prochaines requ\u00eates.</li> </ol>"},{"location":"guides/adapter-prompt/#voir-aussi","title":"Voir aussi","text":"<ul> <li>R\u00e9f\u00e9rence du <code>Modelfile</code></li> <li>Changer de mod\u00e8le LLM</li> </ul>"},{"location":"guides/adapter-prompt/#faq","title":"FAQ","text":""},{"location":"guides/adapter-prompt/#comment-verifier-que-le-nouveau-prompt-est-applique","title":"Comment v\u00e9rifier que le nouveau prompt est appliqu\u00e9 ?","text":"<p>Red\u00e9marrez Ollama avec <code>make down</code> puis <code>make up</code>. Les journaux indiquent le mod\u00e8le et le prompt charg\u00e9s.</p>"},{"location":"guides/adapter-prompt/#peuton-definir-plusieurs-prompts","title":"Peut\u2011on d\u00e9finir plusieurs prompts ?","text":"<p>Non. Le fichier <code>Modelfile</code> ne contient qu'une seule section <code>SYSTEM</code>. Modifiez cette section pour changer de prompt.</p>"},{"location":"guides/changer-modele/","title":"Changer le mod\u00e8le LLM","text":"<p>Ce guide explique comment utiliser un autre mod\u00e8le de langage avec GodotAI.</p> <ol> <li>Ouvrez <code>docker-compose.yml</code> et modifiez la variable <code>OLLAMA_TEXT_MODEL</code> pour indiquer le nom du mod\u00e8le souhait\u00e9.</li> <li>Relancez le service Ollama\u00a0:    <pre><code>make down\nmake up\n</code></pre>    Le nouveau mod\u00e8le est t\u00e9l\u00e9charg\u00e9 puis charg\u00e9 automatiquement.</li> <li>V\u00e9rifiez la liste des mod\u00e8les disponibles\u00a0:    <pre><code>curl http://localhost:11434/api/tags | jq\n</code></pre>    Vous devriez voir le mod\u00e8le choisi dans la liste.</li> </ol>"},{"location":"guides/changer-modele/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Fichier <code>docker-compose.yml</code></li> <li>D\u00e9tails du <code>Modelfile</code></li> <li>D\u00e9pannage mod\u00e8les et GPU</li> </ul>"},{"location":"guides/changer-modele/#faq","title":"FAQ","text":""},{"location":"guides/changer-modele/#pourquoi-mon-modele-nestil-pas-telecharge","title":"Pourquoi mon mod\u00e8le n'est\u2011il pas t\u00e9l\u00e9charg\u00e9 ?","text":"<p>Assurez\u2011vous que <code>OLLAMA_TEXT_MODEL</code> dans <code>docker-compose.yml</code> correspond bien au mod\u00e8le souhait\u00e9 puis ex\u00e9cutez <code>make down</code> suivi de <code>make up</code>.</p>"},{"location":"guides/changer-modele/#puisje-utiliser-un-modele-non-liste","title":"Puis\u2011je utiliser un mod\u00e8le non list\u00e9 ?","text":"<p>Oui, indiquez son nom complet dans <code>OLLAMA_TEXT_MODEL</code> tant qu'il est compatible avec Ollama.</p>"},{"location":"guides/configurer-env/","title":"\u2699\ufe0f Configurer l'environnement","text":"<p>Ce guide explique comment personnaliser les variables d'environnement de GodotAI.</p> <ol> <li>Ouvrez le fichier <code>.env</code> \u00e0 la racine du d\u00e9p\u00f4t.</li> <li>Modifiez les valeurs selon vos besoins\u00a0: mod\u00e8les, ports ou chemin de Godot.</li> <li>Red\u00e9marrez la stack pour appliquer les changements\u00a0:    <pre><code>make down\nmake up\n</code></pre>    Les mod\u00e8les seront t\u00e9l\u00e9charg\u00e9s automatiquement par le conteneur Ollama s'ils ne sont pas encore pr\u00e9sents.</li> </ol> <p>Pour utiliser le GPU, d\u00e9finissez <code>NVIDIA_VISIBLE_DEVICES=all</code> avant de lancer <code>make up</code>.</p>"},{"location":"guides/configurer-env/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Liste des variables</li> </ul>"},{"location":"guides/depannage-modeles-gpu/","title":"D\u00e9pannage des mod\u00e8les et du GPU","text":"<p>Quelques pistes si les mod\u00e8les ne se chargent pas ou si le GPU n'est pas utilis\u00e9\u00a0:</p> <ul> <li>Forcer le re-t\u00e9l\u00e9chargement d'un mod\u00e8le :   <pre><code>docker volume rm godot_ai_ollama_models\nmake up\n</code></pre></li> <li>V\u00e9rifier l'utilisation du GPU :   <pre><code>docker compose logs -f ollama | grep -i gpu\n</code></pre>   ou   <pre><code>docker exec -it ollama nvidia-smi\n</code></pre></li> </ul>"},{"location":"guides/depannage-modeles-gpu/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Changer le mod\u00e8le LLM</li> <li>D\u00e9tails de <code>docker-compose.yml</code></li> </ul>"},{"location":"guides/qualite-redaction-vale/","title":"\u270d\ufe0f Contr\u00f4ler la r\u00e9daction avec Vale","text":"<p>Ce guide explique comment utiliser Vale pour harmoniser la documentation. Vale v\u00e9rifie la grammaire, la ponctuation et applique un ton coh\u00e9rent.</p>"},{"location":"guides/qualite-redaction-vale/#installer-vale","title":"Installer Vale","text":"<p>T\u00e9l\u00e9chargez la derni\u00e8re version depuis GitHub et placez l'ex\u00e9cutable dans votre <code>PATH</code>.</p> <pre><code>curl -L https://github.com/errata-ai/vale/releases/latest/download/vale_3.11.2_Linux_64-bit.tar.gz \\\n  | tar -xz\nsudo mv vale /usr/local/bin/\n</code></pre>"},{"location":"guides/qualite-redaction-vale/#utiliser-le-fichier-valeini","title":"Utiliser le fichier <code>.vale.ini</code>","text":"<p>Le d\u00e9p\u00f4t fournit une configuration personnalis\u00e9e\u00a0:</p> <pre><code>MinAlertLevel = warning\nStylesPath = .vale/styles\n\n[*]\nBasedOnStyles = Vale\nPackages = CoolStyle\n</code></pre> <p>Lancez Vale sur le dossier <code>docs</code> pour v\u00e9rifier tous les articles\u00a0:</p> <pre><code>vale docs/\n</code></pre>"},{"location":"guides/qualite-redaction-vale/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Fichier <code>.vale.ini</code> d\u00e9taill\u00e9</li> </ul>"},{"location":"guides/qualite-redaction-vale/#bonnes-pratiques","title":"Bonnes pratiques","text":"<ul> <li>R\u00e9digez des phrases courtes et directes.</li> <li>Pr\u00e9f\u00e9rez la voix active.</li> <li>\u00c9vitez les adverbes superflus.</li> <li>V\u00e9rifiez les liens et le format Markdown avant de lancer Vale.</li> </ul> <p>En suivant ces conseils, la documentation reste claire et professionnelle.</p>"},{"location":"guides/troubleshooting/","title":"\ud83d\udee0\ufe0f Troubleshooting","text":"<p>Cette page recense les erreurs les plus courantes et comment les r\u00e9soudre.</p>"},{"location":"guides/troubleshooting/#les-conteneurs-ne-demarrent-pas","title":"Les conteneurs ne d\u00e9marrent pas","text":"<ul> <li>Assurez-vous que Docker est en cours d'ex\u00e9cution.</li> <li>Recr\u00e9ez les images :   <pre><code>make rebuild\n</code></pre></li> </ul>"},{"location":"guides/troubleshooting/#lapi-reste-inaccessible","title":"L'API reste inaccessible","text":"<ul> <li>Utilisez le script de v\u00e9rification :   <pre><code>.venv/bin/python utils/test_services.py\n</code></pre></li> <li>Examinez les journaux :   <pre><code>docker compose logs -f\n</code></pre></li> </ul>"},{"location":"guides/troubleshooting/#fastapi-ne-trouve-pas-backendappmain","title":"FastAPI ne trouve pas <code>backend.app.main</code>","text":"<ul> <li>Si les logs affichent <code>ModuleNotFoundError: No module named 'backend.app.main'</code>,   reconstruisez les images :   <pre><code>make rebuild\n</code></pre></li> </ul>"},{"location":"guides/troubleshooting/#erreurs-de-dependances-python","title":"Erreurs de d\u00e9pendances Python","text":"<ul> <li>(R\u00e9)installez les paquets :   <pre><code>make install\n</code></pre></li> </ul>"},{"location":"guides/troubleshooting/#problemes-de-documentation","title":"Probl\u00e8mes de documentation","text":"<ul> <li>Testez la g\u00e9n\u00e9ration :   <pre><code>mkdocs build\n</code></pre></li> <li>V\u00e9rifiez le style :   <pre><code>vale docs/\n</code></pre></li> </ul>"},{"location":"guides/troubleshooting/#modeles-manquants-ou-corrompus","title":"Mod\u00e8les manquants ou corrompus","text":"<ul> <li>Purgez les volumes et relancez :   <pre><code>make cleanall\nmake up\n</code></pre></li> </ul>"},{"location":"guides/troubleshooting/#tout-verifier-dun-coup","title":"Tout v\u00e9rifier d'un coup","text":"<ul> <li>Lancez toutes les v\u00e9rifications :   <pre><code>make universe\n</code></pre>   Le rapport complet se trouve dans <code>rapports/universe.log</code>.</li> </ul>"},{"location":"guides/utiliser-api/","title":"Utiliser l'API REST","text":"<p>Vous pouvez interagir avec GodotAI sans lancer le client Godot en appelant directement le backend FastAPI.</p> <p>Exemple d'appel pour g\u00e9n\u00e9rer du texte\u00a0: <pre><code>curl -X POST http://localhost:8000/txt \\\n     -H 'Content-Type: application/json' \\\n     -d '{\"prompt\": \"Bonjour\"}'\n</code></pre> La r\u00e9ponse est un objet JSON contenant le texte g\u00e9n\u00e9r\u00e9.</p> <p>Pour g\u00e9n\u00e9rer directement une image\u00a0: <pre><code>curl -X POST http://localhost:8000/img \\\n     -H 'Content-Type: application/json' \\\n     -d '{\"prompt\": \"Un village m\u00e9di\u00e9val\"}' -o image.png\n</code></pre></p>"},{"location":"guides/utiliser-api/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Endpoints d\u00e9taill\u00e9s</li> <li>Explications sur FastAPI</li> </ul>"},{"location":"reference/","title":"R\u00e9f\u00e9rence technique","text":"<p>Cette section recense les fichiers essentiels du d\u00e9p\u00f4t et la fa\u00e7on de les utiliser.</p>"},{"location":"reference/#fichiers-principaux","title":"Fichiers principaux","text":"<ul> <li><code>docker-compose.yml</code> : orchestre FastAPI, Ollama et Stable Diffusion.</li> <li><code>Dockerfile.fastapi</code> : construit l'image du backend FastAPI.</li> <li><code>Dockerfile.ollama</code> : pr\u00e9pare le service Ollama et lance <code>entrypoint_ollama.sh</code>.</li> <li><code>Modelfile</code> : d\u00e9finit le prompt syst\u00e8me et les param\u00e8tres du mod\u00e8le.</li> <li><code>Makefile</code> : rassemble les commandes (<code>make up</code>, <code>make down</code>, <code>make docs-serve</code>...).</li> <li><code>mkdocs.yml</code> : configure la documentation.</li> <li><code>variables-env.md</code> : liste compl\u00e8te des variables d'environnement.</li> <li><code>AGENTS.md</code> : d\u00e9crit les conventions de contribution automatiques.</li> </ul>"},{"location":"reference/#tests","title":"Tests","text":"<ul> <li><code>backend/tests/</code> contient les tests unitaires ex\u00e9cut\u00e9s avec <code>pytest</code>.</li> <li><code>e2e/</code> stocke un exemple de test Playwright.</li> <li><code>utils/test_services.py</code> v\u00e9rifie que les conteneurs r\u00e9pondent correctement.</li> </ul>"},{"location":"reference/agents-file/","title":"\ud83d\udcdd AGENTS.md","text":"<p><code>AGENTS.md</code> d\u00e9crit les r\u00f4les et les bonnes pratiques \u00e0 suivre pour contribuer au projet avec un agent automatis\u00e9. On y retrouve notamment : - l\u2019obligation d\u2019ex\u00e9cuter les tests Python avec <code>pytest</code> pour toute modification du backend ; - le formatage du code via <code>black</code> ; - des conventions de commit claires ainsi qu\u2019un message de pull request commen\u00e7ant par l\u2019emoji <code>\ud83e\udd16</code> ; - la g\u00e9n\u00e9ration de la documentation avec <code>mkdocs build</code> apr\u00e8s chaque mise \u00e0 jour.</p> <p>Ces r\u00e8gles assurent une base de code coh\u00e9rente et des contributions faciles \u00e0 relire.</p>"},{"location":"reference/agents-file/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Tests unitaires</li> </ul>"},{"location":"reference/api-backend/","title":"API du backend","text":"M\u00e9thode Endpoint Description GET <code>/</code> V\u00e9rifie que le backend fonctionne GET <code>/txt</code> Test de connexion pour la route texte POST <code>/txt</code> G\u00e9n\u00e8re du texte via Ollama GET <code>/img</code> Retourne une image d'exemple POST <code>/img</code> G\u00e9n\u00e8re une image via Stable Diffusion GET <code>/list_models</code> Liste les mod\u00e8les disponibles POST <code>/users</code> Cr\u00e9e un utilisateur POST <code>/sessions</code> Cr\u00e9e une session de jeu GET <code>/sessions/{id}</code> R\u00e9cup\u00e8re une session POST <code>/mcp</code> Endpoint JSON-RPC MCP <p>L'API d\u00e9marre automatiquement avec les autres services\u00a0:</p> <p><pre><code>make up\n</code></pre> Cette commande lance Docker Compose qui orchestre les conteneurs FastAPI, Ollama et Stable Diffusion.</p>"},{"location":"reference/api-backend/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Guide d'utilisation de l'API</li> </ul>"},{"location":"reference/configuration/","title":"Configuration syst\u00e8me","text":"<p>Principales variables d'environnement\u00a0:</p> <ul> <li><code>OLLAMA_TEXT_HOST</code> : h\u00f4te du service Ollama pour le texte (par d\u00e9faut <code>ollama</code>).</li> <li><code>OLLAMA_TEXT_PORT</code> : port d'\u00e9coute d'Ollama pour le texte (<code>11434</code>).</li> <li><code>STABLEDIFFUSION_HOST</code> : h\u00f4te du service de g\u00e9n\u00e9ration d'images (<code>stablediffusion</code>).</li> <li><code>STABLEDIFFUSION_PORT</code> : port du service d'images (<code>7860</code>).</li> <li><code>OLLAMA_TEXT_MODEL</code> : mod\u00e8le utilis\u00e9 pour la g\u00e9n\u00e9ration de texte.</li> <li><code>STABLEDIFFUSION_MODEL</code> : mod\u00e8le pour la g\u00e9n\u00e9ration d'images.</li> <li><code>NVIDIA_VISIBLE_DEVICES</code> : permet d'activer le GPU dans les conteneurs.</li> <li><code>GODOT_PATH</code> : chemin de l'ex\u00e9cutable Godot.</li> <li><code>DATABASE_URL</code> : cha\u00eene de connexion SQL.</li> <li><code>POSTGRES_USER</code> et <code>POSTGRES_PASSWORD</code> : identifiants PostgreSQL.</li> <li><code>POSTGRES_DB</code> : nom de la base PostgreSQL.</li> <li><code>MONGO_URL</code> : adresse de MongoDB.</li> <li><code>MONGO_DB</code> : base MongoDB utilis\u00e9e.</li> </ul> <p>Cette liste r\u00e9sume les options principales. Consultez variables-env.md pour le tableau complet.</p> <p>Fichiers importants\u00a0:</p> <ul> <li><code>docker-compose.yml</code> orchestre les services.</li> <li><code>Modelfile</code> d\u00e9crit le mod\u00e8le et le prompt syst\u00e8me.</li> <li><code>Dockerfile.ollama</code> construit l'image Ollama personnalis\u00e9e.</li> </ul> <p>Apr\u00e8s toute modification, red\u00e9marrez la stack via <code>make down</code> puis <code>make up</code> pour appliquer les nouvelles valeurs.</p>"},{"location":"reference/configuration/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Changer le mod\u00e8le LLM</li> </ul>"},{"location":"reference/docker-compose-yml/","title":"\ud83d\udc33 docker-compose.yml","text":"<p>Ce fichier coordonne les conteneurs n\u00e9cessaires au projet. Il d\u00e9finit cinq services\u00a0: - fastapi pour le backend Python ; - ollama pour la g\u00e9n\u00e9ration de texte et d\u2019images, construit \u00e0 partir du <code>Dockerfile.ollama</code> ; - stablediffusion pour l\u2019interface Web de Stable Diffusion ; - postgres pour la base relationnelle ; - mongo pour stocker les r\u00e9ponses compl\u00e8tes. Les volumes nomm\u00e9s, comme <code>ollama_models</code>, <code>sd_models</code> ou <code>postgres_data</code>, conservent les donn\u00e9es entre chaque ex\u00e9cution. Les variables sont d\u00e9finies dans <code>.env</code>. En r\u00e8gle g\u00e9n\u00e9rale, <code>docker-compose.yml</code> permet de d\u00e9marrer l\u2019ensemble avec <code>docker compose up</code> ou la commande <code>make up</code> pr\u00e9vue dans le projet.</p>"},{"location":"reference/docker-compose-yml/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Guide pour changer de mod\u00e8le</li> <li>Explications sur Docker Compose</li> </ul>"},{"location":"reference/docker-compose-yml/#faq","title":"FAQ","text":""},{"location":"reference/docker-compose-yml/#peuton-modifier-les-ports-exposes","title":"Peut\u2011on modifier les ports expos\u00e9s ?","text":"<p>Oui. Mettez \u00e0 jour les variables correspondantes dans <code>.env</code> puis relancez <code>make up</code>.</p>"},{"location":"reference/docker-compose-yml/#comment-ajouter-un-service-supplementaire","title":"Comment ajouter un service suppl\u00e9mentaire ?","text":"<p>D\u00e9clarez-le dans <code>docker-compose.yml</code> et compl\u00e9tez \u00e9ventuellement le <code>Makefile</code>.</p>"},{"location":"reference/dockerfile-ollama/","title":"\ud83d\udc0b Dockerfile.ollama","text":"<p>Bas\u00e9 sur l\u2019image officielle <code>ollama/ollama</code>, ce Dockerfile ajoute quelques outils pratiques comme <code>curl</code> et <code>pciutils</code>. Il int\u00e8gre \u00e9galement le <code>Modelfile</code> copi\u00e9 dans l'image et le script <code>entrypoint_ollama.sh</code> qui g\u00e8re la cr\u00e9ation du mod\u00e8le personnalis\u00e9 <code>god</code> ainsi que le t\u00e9l\u00e9chargement automatique des autres mod\u00e8les.</p> <pre><code>FROM ollama/ollama:latest\nRUN apt-get update &amp;&amp; apt-get install -y curl pciutils\nCOPY Modelfile /Modelfile\nCOPY entrypoint_ollama.sh /entrypoint_ollama.sh\nENTRYPOINT [\"/entrypoint_ollama.sh\"]\n</code></pre> <p>Le <code>Modelfile</code> est embarqu\u00e9 dans l'image. Lors du premier d\u00e9marrage, <code>entrypoint_ollama.sh</code> utilise ce fichier pour g\u00e9n\u00e9rer le mod\u00e8le local <code>god</code> dans le volume <code>ollama_models</code>.</p> <p>L\u2019entr\u00e9e <code>ENTRYPOINT</code> lance ce script pour s\u2019assurer que les mod\u00e8les pr\u00e9cis\u00e9s sont bien pr\u00e9sents avant d\u2019exposer l\u2019API Ollama.</p> <ul> <li>Les mod\u00e8les \u00e0 r\u00e9cup\u00e9rer sont d\u00e9finis par <code>OLLAMA_TEXT_MODEL</code> et <code>STABLEDIFFUSION_MODEL</code> dans <code>docker-compose.yml</code>.</li> <li>Ils sont enregistr\u00e9s dans le volume Docker <code>ollama_models</code> afin d\u2019\u00e9viter des t\u00e9l\u00e9chargements r\u00e9p\u00e9t\u00e9s.</li> </ul> <p>Ce conteneur se combine ensuite avec le service FastAPI via <code>docker compose up</code>.</p> <p>Lancez la stack et la construction de l'image avec <code>make up</code>.</p>"},{"location":"reference/dockerfile-ollama/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Script <code>entrypoint_ollama.sh</code></li> <li>Fichier <code>Modelfile</code></li> </ul>"},{"location":"reference/dockerfile/","title":"\ud83d\udc0b Dockerfile.fastapi","text":"<p>Le <code>Dockerfile.fastapi</code> construit l'image du backend FastAPI. Bas\u00e9e sur <code>python:3.11-slim</code>, elle installe les d\u00e9pendances list\u00e9es dans <code>backend/requirements.txt</code> puis copie le code du dossier <code>backend</code>.</p> <p>Le conteneur lance ensuite Uvicorn sur le port 8000\u00a0: <pre><code>CMD [\"uvicorn\", \"backend.app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre> Ce service est ensuite d\u00e9marr\u00e9 par <code>docker-compose.yml</code> sous le nom fastapi.</p> <p>Reb\u00e2tissez cette image et relancez FastAPI avec <code>make rebuild</code>.</p>"},{"location":"reference/dockerfile/#voir-aussi","title":"Voir aussi","text":"<ul> <li><code>docker-compose.yml</code></li> </ul>"},{"location":"reference/entrypoint-ollama/","title":"\ud83d\udd27 entrypoint_ollama.sh","text":"<p>Ce script s'ex\u00e9cute lorsque le conteneur Ollama d\u00e9marre. Son r\u00f4le est de pr\u00e9parer l'environnement avant d'exposer l'API.</p> <ol> <li>Lancement de <code>ollama serve</code> en arri\u00e8re\u2011plan.</li> <li>Cr\u00e9ation du mod\u00e8le local <code>god</code> \u00e0 partir du <code>Modelfile</code> si n\u00e9cessaire.</li> <li>V\u00e9rification de la pr\u00e9sence des mod\u00e8les sp\u00e9cifi\u00e9s dans <code>OLLAMA_TEXT_MODEL</code> et <code>STABLEDIFFUSION_MODEL</code>.</li> <li>T\u00e9l\u00e9chargement automatique via <code>ollama pull</code> si un mod\u00e8le manque.</li> <li>Affichage d'une barre de progression pour suivre l'avancement.</li> <li>Attente bloquante tant que <code>ollama serve</code> est actif.</li> </ol> <p>Gr\u00e2ce \u00e0 cette s\u00e9quence, le mod\u00e8le <code>god</code> est stock\u00e9 dans le volume <code>ollama_models</code> et le service est pr\u00eat \u00e0 r\u00e9pondre d\u00e8s le premier <code>docker compose up</code>.</p>"},{"location":"reference/entrypoint-ollama/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Dockerfile.ollama</li> <li>Modelfile</li> </ul>"},{"location":"reference/gitignore/","title":"\ud83d\ude48 .gitignore","text":"<p>Ce fichier liste tout ce qui doit rester en dehors du suivi Git : les dossiers propres \u00e0 Godot, les caches Python et les fichiers temporaires. En ignorant ces \u00e9l\u00e9ments, on \u00e9vite d\u2019encombrer le d\u00e9p\u00f4t avec des donn\u00e9es g\u00e9n\u00e9r\u00e9es automatiquement.</p> <p>Principales entr\u00e9es : - <code>.godot/</code> et <code>.import/</code> pour ne pas versionner les fichiers g\u00e9n\u00e9r\u00e9s par l\u2019\u00e9diteur. - <code>__pycache__/</code> et les fichiers <code>*.pyc</code> issus de Python. - Le dossier <code>ollama_models</code> et autres volumes Docker ne sont pas suivis pour pr\u00e9server l\u2019espace du r\u00e9pertoire Git.</p> <p>En r\u00e9sum\u00e9, <code>.gitignore</code> garde le d\u00e9p\u00f4t propre et l\u00e9ger.</p>"},{"location":"reference/gitignore/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Configuration des volumes</li> </ul>"},{"location":"reference/makefile/","title":"\ud83d\udee0 Makefile","text":"<p>Le <code>Makefile</code> centralise plusieurs commandes utiles : - <code>make up</code> lance tous les conteneurs via Docker Compose ; - <code>make down</code> arr\u00eate les conteneurs ; - <code>make rebuild</code> recr\u00e9e les images Docker sans cache ; - <code>make clean</code> supprime les caches Python ; - <code>make cleanall</code> supprime aussi les volumes Docker pour repartir de z\u00e9ro ; - <code>make purge-models</code> supprime les mod\u00e8les enregistr\u00e9s dans les volumes Docker ; - <code>make up-models</code> lance la stack en choisissant <code>MODEL_TEXT</code> et <code>MODEL_IMAGE</code>,   puis affiche les noms retenus ; - <code>make docs-serve</code> pr\u00e9visualise la documentation ; - <code>make docs-deploy</code> la publie sur GitHub Pages. - <code>make universe</code> ex\u00e9cute tous les tests et g\u00e9n\u00e8re <code>rapports/universe.log</code>.</p> <p>En r\u00e9sum\u00e9, il simplifie le quotidien en \u00e9vitant de longues lignes de commande.</p>"},{"location":"reference/makefile/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Fichier <code>docker-compose.yml</code></li> <li>Tests unitaires</li> </ul>"},{"location":"reference/mkdocs-yml/","title":"\ud83d\udcda mkdocs.yml","text":"<p>Ce fichier configure le site de documentation g\u00e9n\u00e9r\u00e9 par MkDocs.</p> <p>Principales sections\u00a0:</p> <ul> <li><code>site_name</code> et <code>site_url</code> d\u00e9finissent le nom du projet et son adresse en ligne.</li> <li><code>theme</code> indique le th\u00e8me Material utilis\u00e9 pour le rendu.</li> <li><code>nav</code> liste l'ensemble des pages et leur organisation dans la barre de navigation.</li> <li><code>markdown_extensions</code> active SuperFences pour les blocs de code.</li> <li>Les diagrammes D2 sont g\u00e9n\u00e9r\u00e9s s\u00e9par\u00e9ment en SVG et int\u00e9gr\u00e9s comme images.</li> </ul> <p>Apr\u00e8s chaque ajout de page, ex\u00e9cutez\u00a0:</p> <pre><code>mkdocs build\n</code></pre> <p>Cette commande s'assure que la documentation est valide avant une \u00e9ventuelle publication.</p> <p>Pour faciliter ces actions, utilisez <code>make docs-serve</code> pour un aper\u00e7u local et <code>make docs-deploy</code> pour la publication.</p>"},{"location":"reference/mkdocs-yml/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Pr\u00e9sentation de MkDocs</li> </ul>"},{"location":"reference/modelfile/","title":"\ud83d\udcd1 Modelfile","text":"<p><code>Modelfile</code> d\u00e9finit quel mod\u00e8le Ollama doit charger et le prompt syst\u00e8me \u00e0 appliquer.</p> <p>Extrait simplifi\u00e9 : <pre><code>FROM mistral:7b-instruct\nPARAMETER temperature 0.5\nPARAMETER num_ctx 200\nSYSTEM \"\"\"\nTu es un ma\u00eetre du jeu francophone...\n\"\"\"\n</code></pre></p> <p>La directive <code>FROM</code> choisit la base du mod\u00e8le. Les lignes <code>PARAMETER</code> ajustent son comportement, par exemple la cr\u00e9ativit\u00e9 (<code>temperature</code>) ou la taille du contexte (<code>num_ctx</code>). Le bloc <code>SYSTEM</code> contient le prompt envoy\u00e9 au mod\u00e8le \u00e0 chaque requ\u00eate.</p> <p>Ce fichier est copi\u00e9 dans l'image Ollama construite via <code>Dockerfile.ollama</code>. En le modifiant, on peut changer de mod\u00e8le ou personnaliser l'exp\u00e9rience de jeu.</p> <p>Lors de la construction de l'image, la commande suivante g\u00e9n\u00e8re le mod\u00e8le local\u202f:</p> <pre><code>ollama serve &amp;\nuntil curl -s http://127.0.0.1:11434/api/tags &gt; /dev/null; do sleep 1; done\nollama create god -f /Modelfile\nkill $!\n</code></pre>"},{"location":"reference/modelfile/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Guide pour adapter le prompt</li> </ul>"},{"location":"reference/modelfile/#faq","title":"FAQ","text":""},{"location":"reference/modelfile/#ou-se-trouve-ce-fichier-dans-le-conteneur","title":"O\u00f9 se trouve ce fichier dans le conteneur ?","text":"<p>Il est int\u00e9gr\u00e9 \u00e0 l'image Ollama lors de la construction. Toute modification n\u00e9cessite donc un red\u00e9marrage via <code>make up</code> pour \u00eatre prise en compte.</p>"},{"location":"reference/modelfile/#peuton-utiliser-plusieurs-modelfiles","title":"Peut\u2011on utiliser plusieurs Modelfiles ?","text":"<p><code>docker-compose.yml</code> fait r\u00e9f\u00e9rence \u00e0 un unique <code>Modelfile</code>. Pour tester plusieurs prompts, modifiez ce fichier puis red\u00e9marrez Ollama avec <code>make down</code> puis <code>make up</code> pour prendre en compte les changements.</p>"},{"location":"reference/test-services/","title":"\ud83d\udee0\ufe0f test_services.py","text":"<p><code>test_services.py</code> sert \u00e0 v\u00e9rifier que FastAPI, Ollama et Stable Diffusion r\u00e9pondent bien une fois les conteneurs d\u00e9marr\u00e9s.</p> <ol> <li>D\u00e9marrez les services avec <code>make up</code>.</li> <li>Installez les d\u00e9pendances Python si n\u00e9cessaire\u00a0:    <pre><code>make install\n</code></pre></li> <li>Lancez le script\u00a0:    <pre><code>.venv/bin/python utils/test_services.py\n</code></pre></li> </ol> <p>Ce test est \u00e0 ex\u00e9cuter juste apr\u00e8s <code>make up</code> pour vous assurer que la stack est op\u00e9rationnelle. Il affiche pour chaque service s'il est joignable. Un emoji indique l'\u00e9tat\u00a0: <code>\u2705</code> si tout fonctionne, <code>\u23f3</code> lorsque Stable Diffusion charge encore son mod\u00e8le et <code>\u274c</code> en cas d'erreur. Le script renvoie un code de sortie non nul si un service est indisponible.</p>"},{"location":"reference/test-services/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Tutoriel de prise en main</li> </ul>"},{"location":"reference/tests-e2e/","title":"\ud83d\udea6 Tests E2E avec Playwright","text":"<p>Le projet propose \u00e9galement un exemple de test de bout en bout utilisant Playwright. Ces tests vivent dans le dossier <code>e2e/</code>.</p> <p>Pour installer Playwright et ses d\u00e9pendances :</p> <pre><code>make install\n# Optionnel : .venv/bin/playwright install\n</code></pre> <p>Lancez ensuite les tests E2E avec :</p> <pre><code>pytest e2e\n</code></pre> <p>Le script d\u00e9marre bri\u00e8vement le serveur FastAPI sur un port local puis effectue un appel HTTP via l'API <code>request</code> de Playwright.</p> <p>Extrait du fichier <code>test_api_playwright.py</code>\u00a0:</p> <pre><code>from playwright.sync_api import sync_playwright\n\ndef test_root_endpoint():\n    with sync_playwright() as p:\n        request = p.request.new_context(base_url=\"http://127.0.0.1:8002\")\n        resp = request.get(\"/\")\n        assert resp.status == 200\n</code></pre>"},{"location":"reference/tests-e2e/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Tests unitaires</li> <li>Guide de d\u00e9marrage</li> </ul>"},{"location":"reference/tests-unitaires/","title":"\u2705 Tests unitaires","text":"<p>Cette page explique comment lancer les tests unitaires du backend.</p> <p>Les tests sont \u00e9crits avec pytest et se trouvent dans le dossier <code>backend/tests/</code>. Pour ex\u00e9cuter l'ensemble des tests :</p> <pre><code>pytest -q\n</code></pre> <p>Exemple de test issu du fichier <code>test_root.py</code>\u00a0:</p> <pre><code>from fastapi.testclient import TestClient\nfrom backend.app.main import app\n\nclient = TestClient(app)\n\ndef test_read_root():\n    resp = client.get(\"/\")\n    assert resp.status_code == 200\n    assert resp.json()[\"message\"] == \"Backend FastAPI fonctionne !\"\n</code></pre> <p>Pour cibler un seul test pendant le d\u00e9veloppement\u00a0:</p> <pre><code>pytest backend/tests/test_root.py::test_read_root -q\n</code></pre> <p>Les d\u00e9pendances n\u00e9cessaires sont list\u00e9es dans <code>backend/requirements.txt</code>. Avant de lancer les tests pour la premi\u00e8re fois, installez-les avec :</p> <pre><code>make install\n</code></pre> <p>Chaque nouvelle fonctionnalit\u00e9 Python doit \u00eatre accompagn\u00e9e d'un test correspondant dans ce r\u00e9pertoire.</p>"},{"location":"reference/tests-unitaires/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Tests E2E</li> <li>AGENTS.md</li> </ul>"},{"location":"reference/vale/","title":"\ud83d\udd0d .vale.ini","text":"<p><code>Vale</code> est un outil de lint pour la r\u00e9daction. Le projet active un style personnalisable pour un ton plus direct et coh\u00e9rent.</p> <p>Contenu principal : - <code>MinAlertLevel = warning</code> : ne signale que les r\u00e8gles importantes. - <code>StylesPath = .vale/styles</code> : localisation des r\u00e8gles maison. - <code>BasedOnStyles = Vale</code> : conserve le jeu de r\u00e8gles standard. - <code>Packages = CoolStyle</code> : applique nos r\u00e8gles suppl\u00e9mentaires.</p> <p>Ainsi, <code>.vale.ini</code> permet de v\u00e9rifier rapidement les textes sans configuration complexe.</p>"},{"location":"reference/vale/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Guide d'utilisation de Vale</li> </ul>"},{"location":"reference/variables-env/","title":"\ud83c\udf21\ufe0f Variables d'environnement","text":"<p>Le fichier <code>.env</code> centralise la configuration de l'ensemble des services. Voici la liste des variables disponibles\u00a0:</p> Variable Valeur par d\u00e9faut R\u00f4le <code>OLLAMA_TEXT_MODEL</code> <code>god:latest</code> Mod\u00e8le de g\u00e9n\u00e9ration de texte. <code>OLLAMA_TEXT_HOST</code> <code>ollama</code> H\u00f4te du service Ollama pour le texte. <code>OLLAMA_TEXT_PORT</code> <code>11434</code> Port d'Ollama pour les requ\u00eates texte. <code>STABLEDIFFUSION_MODEL</code> <code>stable-diffusion</code> Mod\u00e8le de g\u00e9n\u00e9ration d'images. <code>STABLEDIFFUSION_HOST</code> <code>stablediffusion</code> H\u00f4te pour la g\u00e9n\u00e9ration d'images. <code>STABLEDIFFUSION_PORT</code> <code>7860</code> Port du service d'images. <code>GODOT_PATH</code> <code>./Godot_v4.x86_64</code> Ex\u00e9cutable Godot utilis\u00e9 par le <code>Makefile</code>. Si absent, <code>godot4</code> est essay\u00e9. <code>DATABASE_URL</code> <code>sqlite:///./data/game.db</code> Cha\u00eene de connexion SQL. <code>POSTGRES_USER</code> <code>postgres</code> Utilisateur de la base PostgreSQL. <code>POSTGRES_PASSWORD</code> <code>postgres</code> Mot de passe PostgreSQL. <code>POSTGRES_DB</code> <code>godotai</code> Nom de la base PostgreSQL. <code>MONGO_URL</code> <code>mongodb://mongo:27017</code> Adresse de la base MongoDB. <code>MONGO_DB</code> <code>godotai</code> Nom de la base MongoDB. <code>NVIDIA_VISIBLE_DEVICES</code> (vide) Active l'acc\u00e9l\u00e9ration GPU dans les conteneurs. <p>Ces valeurs sont charg\u00e9es automatiquement par Docker Compose et le <code>Makefile</code>.</p>"},{"location":"reference/variables-env/#voir-aussi","title":"Voir aussi","text":"<ul> <li>Configurer l'environnement</li> </ul>"},{"location":"tutoriels/premiers-pas/","title":"\ud83d\ude80 Premiers pas","text":"<p>Suivez les \u00e9tapes ci-dessous dans l'ordre pour d\u00e9ployer la stack compl\u00e8te.</p> <ol> <li> <p>Pr\u00e9parez les variables d'environnement en copiant le fichier <code>.env</code> fourni puis ajustez les valeurs selon vos besoins\u00a0:    <pre><code>cp .env .env.local\n# OLLAMA_TEXT_MODEL=god:latest\n# STABLEDIFFUSION_MODEL=stable-diffusion\n</code></pre>    Consultez la r\u00e9f\u00e9rence pour la liste compl\u00e8te des variables.</p> </li> <li> <p>Installez Docker et Git.</p> </li> <li>Clonez le d\u00e9p\u00f4t :    <pre><code>git clone &lt;repo_url&gt;\ncd godot_ai\n</code></pre></li> <li>Installez les d\u00e9pendances Python puis v\u00e9rifiez la configuration\u00a0:    <pre><code>make install\n.venv/bin/python utils/test_services.py\n</code></pre>    Ce script confirme que chaque service est joignable.</li> <li>D\u00e9marrez les services :    <pre><code>make up\n</code></pre>    Les mod\u00e8les se t\u00e9l\u00e9chargent automatiquement au premier d\u00e9marrage. Utilisez    <code>make up-models MODEL_TEXT=mistral:7b MODEL_IMAGE=stable-diffusion</code> pour    sp\u00e9cifier d'autres mod\u00e8les. Les noms s\u00e9lectionn\u00e9s s'affichent alors en clair.</li> </ol> <p>Lors de cette \u00e9tape, le conteneur Ollama r\u00e9cup\u00e8re le mod\u00e8le indiqu\u00e9 dans    le <code>Modelfile</code> tandis que Stable Diffusion t\u00e9l\u00e9charge ses poids si    n\u00e9cessaire. Cette op\u00e9ration peut prendre plusieurs minutes mais n'a lieu    qu'une seule fois.    Le script <code>entrypoint_ollama.sh</code> lance <code>ollama serve</code> puis v\u00e9rifie la    pr\u00e9sence des mod\u00e8les. S'ils sont absents, il ex\u00e9cute <code>ollama pull</code> pour les    r\u00e9cup\u00e9rer avant de poursuivre l'initialisation.    Pour plus de d\u00e9tails, consultez test_services.py. 6. (Optionnel) Lancez Godot :    <pre><code>make godot\n</code></pre> 7. (Optionnel) Ex\u00e9cutez les tests unitaires et E2E :    <pre><code>pytest -q\npytest e2e\n</code></pre> 8. Coupez les conteneurs :    <pre><code>make down\n</code></pre></p> <p>Tous les outils mentionn\u00e9s disposent de liens vers leur site officiel et leur documentation sur les pages correspondantes.</p> <p>Une fois ces \u00e9tapes termin\u00e9es, vous pouvez explorer les guides pratiques pour personnaliser le projet. Acc\u00e9der aux guides</p>"}]}