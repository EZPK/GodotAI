services:
  ollama:
    build:
      context: .
      dockerfile: Dockerfile.ollama
    container_name: ${OLLAMA_TEXT_HOST}
    ports:
      - "${OLLAMA_TEXT_PORT}:11434"
    volumes:
      # Volume Docker pour persister les modèles téléchargés
      - ollama_models:/root/.ollama
    env_file: .env
    environment:
      - OLLAMA_HOST=0.0.0.0
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_LLM_LIBRARY=cuda
    restart: unless-stopped
    gpus: all

  stablediffusion:
    image: universonic/stable-diffusion-webui:full
    container_name: ${OLLAMA_IMAGE_HOST}
    ports:
      - "${OLLAMA_IMAGE_PORT}:8080"
    volumes:
      - sd_models:/app/stable-diffusion-webui/models
      - sd_outputs:/app/stable-diffusion-webui/outputs
    env_file: .env
    restart: unless-stopped
    gpus: all

volumes:
  ollama_models:
  sd_models:
  sd_outputs:
