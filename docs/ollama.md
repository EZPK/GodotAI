# ü¶ô Ollama

Ollama lance le mod√®le de langage √† l'int√©rieur d'un conteneur Docker. Il t√©l√©charge
le mod√®le choisi lors du premier d√©marrage (par exemple Mistral ou Llama¬†3).
Ce t√©l√©chargement s'effectue automatiquement lorsque vous ex√©cutez `make up` et
le mod√®le reste ensuite disponible localement pour les d√©marrages suivants.

Le fichier `Modelfile` √† la racine du d√©p√¥t indique quel mod√®le charger. FastAPI
lui envoie les requ√™tes de l'utilisateur pour obtenir une r√©ponse adapt√©e √† la
partie en cours.

Exemple d'ex√©cution manuelle :
```bash
docker run -p 11434:11434 ollama/ollama:latest serve
```

## Ressources
- [Site officiel](https://ollama.ai/)
- [Documentation](https://ollama.ai/docs)
