# ü¶ô Ollama

Ollama lance le mod√®le de langage √† l'int√©rieur d'un conteneur Docker. Il t√©l√©charge
le mod√®le choisi lors du premier d√©marrage (par exemple Mistral ou Llama¬†3).
Ce t√©l√©chargement s'effectue automatiquement lorsque vous ex√©cutez `make up` et
le mod√®le reste ensuite disponible localement pour les d√©marrages suivants.

Le processus est pilot√© par le script `entrypoint_ollama.sh`¬†:
1. Il d√©marre `ollama serve` en arri√®re-plan et attend que l'API r√©ponde.
2. Il v√©rifie la pr√©sence des mod√®les list√©s dans les variables d'environnement
   `OLLAMA_TEXT_MODEL` et `OLLAMA_IMAGE_MODEL` via `ollama list`.
3. Si l'un d'eux est absent, `ollama pull` est ex√©cut√© pour le t√©l√©charger.
4. Enfin, le script laisse tourner `ollama serve` pour traiter les requ√™tes.

Les mod√®les restent dans le volume Docker `ollama_models`, √©vitant ainsi de
nouveaux t√©l√©chargements aux lancements suivants.

Le fichier `Modelfile` √† la racine du d√©p√¥t indique quel mod√®le charger. FastAPI
lui envoie les requ√™tes de l'utilisateur pour obtenir une r√©ponse adapt√©e √† la
partie en cours.

## Voir aussi

- [Fichier `Modelfile`](../reference/modelfile.md)
- [Changer de mod√®le](../guides/changer-modele.md)

![Dialogue avec Ollama](../assets/ollama.svg)

Exemple d'ex√©cution manuelle :
```bash
docker run -p 11434:11434 ollama/ollama:latest serve
```

## Ressources
- [Site officiel](https://ollama.ai/)
- [Documentation](https://ollama.ai/docs)
